#summary Description of the NDT test methodology

=NDT Test Methodology= 

== Abstract == 

The Network Diagnostic Tool (NDT) is a client/server program that provides network configuration and performance testing to a user's computer. The NDT is designed to identify both performance problems and configuration problems. Performance problems affect the user experience, usually causing data transfers to take longer than expected. These problems are usually solved by tuning various TCP (Transmission Control Protocol) network parameters on the end host. Configuration problems also affect the user experience; however, tuning will not improve the end-to-end performance. The configuration fault must be found and corrected to change the end host behavior. The NDT is providing enough information to accomplish these tasks. This document describes how these information is gathered and what NDT is and is not capable of answering.

== Table of Contents ==

<wiki:toc max_depth="3" />

== Introduction ==

The NDT is a typical memory to memory client/server test device. Throughput measurements closely measure the network performance, and ignore the disk I/O effects. The real strength is in the advanced diagnostic features that are enabled by the kernel data automatically collected by the web100 monitoring infrastructure.  This data is collected during the test (at 5 msec increments) and analyzed after the test completes to determine what, if anything, impacted the test. One of the MAJOR issues facing a commodity Internet users is the performance limiting host configuration settings for the Windows XP operating system. To illustrate this, a cable modem user with basic service (15 Mbps download) would MAX out at 13 Mbps with a 40 msec RTT delay. Thus unless the ISP proxies content, the majority of traffic will be limited by the clients configuration and NOT the ISP's infrastructure.  The NDT server can detect and report this problem, saving consumers and ISP's dollars by allowing them to quickly identify where to start looking for a problem. The FCC really needs to understand this message, or we will not be as effective as we need to be.

The NDT operates on any client with a Java-enabled Web browser; further:
 * What it can do:
   * Positively state if Sender, Receiver, or Network is operating properly
   * Provide accurate application tuning info
   * Suggest changes to improve performance
 * What it canâ€™t do:
   * Tell you where in the network the problem is
   * Tell you how other servers perform
   * Tell you how other clients will perform

== Performed tests ==

=== Middlebox Test ===

The middlebox test is a short throughput test from the server to the client with a limited CWND to check for a duplex mismatch condition. Moreover, this test uses a pre-defined MSS value to check if any intermediate node will modify its connection settings.

A detailed description of all of the MID protocol messages can be found in the [NDTProtocol#Middlebox_test NDT Protocol document].

As a first step the server binds an ephemeral port and notify the client about this port number. The server also sets MSS on this port to 1456 (strange value). 

Next, the client connects to the server's ephemeral port. When the connection is successfully established, the server sets the maximum value of the congestion window for this connection to `2 * (The current maximum segment size (MSS))`.

In the next step the server starts a 5 seconds throughput test using the newly created connection. The NDT server sends packets as fast as possible (i.e. without any delays) during the test. These packets are written using the buffer of the following size: `(The current maximum segment size (MSS))`. If such buffer cannot be used, then the server uses a 8192 Byte one. The buffer contains a pre-generated pseudo random data (including only US-ASCII printable characters).

The server can temporarily stop sending packets when the following formula is fulfilled:
{{{
BUFFER_SIZE * 16 < ((Next Sequence Number To Be Sent) - (Oldest Unacknowledged Sequence Number) - 1)
}}}

When the 5 seconds throughput test is over, the server sends the following results to the client:

|| CurMSS || The current maximum segment size (MSS), in octets.||
|| !WinScaleSent || The value of the transmitted window scale option if one was sent; otherwise, a value of -1. ||
|| !WinScaleRcvd || The value of the received window scale option if one was received; otherwise, a value of -1. ||

Next, the client sends its calculated throughput value to the server. This throughput value is calculated using the following formula:
{{{
THROUGHPUT_VALUE = 8 * TRANSMITTED_BYTES / 1000 / TEST_DURATION_SECONDS
}}}

=== Simple Firewall Test ===

The simple firewall test tries to find out any firewalls between the NDT client and the NDT server that will prevent connections to an ephemeral port numbers. The test is performed in both directions (i.e. the NDT client is trying to connect to the NDT server and the NDT server is trying to connect to the NDT client).

A detailed description of all of the SFW protocol messages can be found in the [NDTProtocol#Simple_firewall_test NDT Protocol document].

As a first step both NDT components (the server and the client) bind an ephemeral port and notify the second component about this port number. In the second step both NDT components are executing in parallel:
 # The client is trying to connect to the server's ephemeral port and send a TEST_MSG message containing a pre-defined string "Simple firewall test" of length 20 using the newly created connection.
 # The server is trying to connect to the client's ephemeral port and send a TEST_MSG message containing a pre-defined string "Simple firewall test" of length 20 using the newly created connection.

Both client and server are waiting for a valid connection a limited amount of time. If the MaxRTT or MaxRTO is greater than 3 seconds, than the time limit in the SFW test is 3 seconds. Otherwise the time limit in the SWF test is 1 second.

The test is finished after the connection will be accepted or the time limit will be exceeded. If the time limit is exceeded, the firewall probably exists somewhere on the end-to-end path. If there is a connection and the pre-defined string is properly transferred, then there is no firewall on the end-to-end path. The third possibility is that there is a successful connection, but the expected pre-defined string is not transferred. This case does not adjudicate about the firewall existence.

In the last step the server sends its results to the client.

The possible simple firewall test result codes:

|| *Value* || *Description* ||
|| "0" || Test was not started ||
|| "1" || Test was successful (i.e. connection to the ephemeral port was possible) ||
|| "2" || There was a connection to the ephemeral port, but it was not recognized properly ||
|| "3" || There was no connection to the ephemeral port within the specified time ||

=== C2S Throughput Test ===

The C2S throughput test tests the achievable network bandwidth from the client to the server by performing a 10 seconds memory-to-memory data transfer.

A detailed description of all of the C2S protocol messages can be found in the [NDTProtocol#C2S_throughput_test NDT Protocol document].

As a first step the server binds an ephemeral port and notify the client about this port number.

Next, the client connects to the server's ephemeral port. When the connection is successfully established, the server initializes the following routines:
 * libpcap routines to perform packet trace used by the Bottleneck Link Detection algorithm.
 * [NDTDataFormat#tcpdump_trace tcpdump trace] to dump all packets sent during the C2S throughput test on the newly created connection. This tcpdump trace dump is only started when the `-t, --tcpdump` options are set.
 * [NDTDataFormat#web100_snaplog_trace web100 snaplog trace] to dump web100 kernel MIB variables' values written in a fixed time (default is 5 msec) increments during the C2S throughput test for the newly created connection. This snaplog trace dump is only started when the `--snaplog` option is set.

In the next step the client starts a 10 seconds throughput test using the newly created connection. The NDT client sends packets as fast as possible (i.e. without any delays) during the test. These packets are written using the 8192 Byte buffer containing a pre-generated pseudo random data (including only US-ASCII printable characters).

When the 10 seconds throughput test is over, the server sends its calculated throughput value to the client. This throughput value is calculated using the following formula:
{{{
THROUGHPUT_VALUE = 8 * TRANSMITTED_BYTES / 1000 / TEST_DURATION_SECONDS
}}}

=== S2C Throughput Test ===

The S2C throughput test tests the achievable network bandwidth from the server to the client by performing a 10 seconds memory-to-memory data transfer.

A detailed description of all of the S2C protocol messages can be found in the [NDTProtocol#S2C_throughput_test NDT Protocol document].

As a first step the server binds an ephemeral port and notify the client about this port number.

Next, the client connects to the server's ephemeral port. When the connection is successfully established, the server initializes the following routines:
 * libpcap routines to perform packet trace used by the Bottleneck Link Detection algorithm.
 * [NDTDataFormat#tcpdump_trace tcpdump trace] to dump all packets sent during the S2C throughput test on the newly created connection. This tcpdump trace dump is only started when the `-t, --tcpdump` options are set.
 * [NDTDataFormat#web100_snaplog_trace web100 snaplog trace] to dump web100 kernel MIB variables' values written in a fixed time (default is 5 msec) increments during the S2C throughput test for the newly created connection. This snaplog trace dump is only started when the `--snaplog` option is set.

In the next step the server starts a 10 seconds throughput test using the newly created connection. The NDT server sends packets as fast as possible (i.e. without any delays) during the test. These packets are written using the 8192 Byte buffer containing a pre-generated pseudo random data (including only US-ASCII printable characters).

When the 10 seconds throughput test is over, the server sends to the client its calculated throughput value, amount of unsent data in the socket send queue and overall number of sent bytes. The throughput value is calculated using the following formula:
{{{
THROUGHPUT_VALUE = 8 * TRANSMITTED_BYTES / 1000 / TEST_DURATION_SECONDS
}}}

Additionally, at the end of the S2C throughput test, the server also takes a web100 snapshot and sends all the web100 data variables to the client.

== Specific detection algorithms ==

=== Bottleneck Link Detection ===

=== Duplex Mismatch Detection ===

=== Link Type Detection Heuristics ===

The following link type detection heuristics are run only when there is no duplex mismatch condition detected and the measured throughput value is the same or smaller than the estimate (which is a correct situation).

==== DSL/Cable modem ====

The link is treated as a DSL/Cable modem when all of the following conditions are met:
 * The cumulative time spent in the 'Sender Limited' state *is less than 0.6 ms*
 * The number of transitions into the 'Congestion Limited' state *is 0*
 * The web100 measured speed *is less than 2000000 bits per second*
 * The web100 measured speed *is less than estimate*

==== IEEE 802.11 (!WiFi) ====

==== Ethernet link (Fast Ethernet) ====

The link is treated as an Ethernet link (Fast Ethernet) when all of the following conditions are met:
 * The heuristics for !WiFi and DSL/Cable modem links *give negative results*
 * The web100 measured speed *is less than 9500000 bits per second*
 * The web100 measured speed *is greater than 3000000 bits per second*
 * The S2C throughput test measured speed *is less than 9500000 bits per second*
 * The proportion of the number of multiplicative downward congestion window adjustments to the total number of segments sent *is less than 1%*
 * The proportion of the duplicate ACKs received to the overall number of valid pure ACKs received *is less than 3.5%*

=== Faulty Hardware Link Detection ===

=== Full/Half Link Duplex Setting ===

=== Normal Congestion Detection ===

=== Firewall Detection ===

=== NAT Detection ===

=== Abnormal Conditions Detection ===

 * high packet loss
 * packets arriving out of order

=== Latency/Jitter ===

 * Multiple measurements of the round trip delay and per-packet transmission/arrival times are collected.
 * The delay for each measurement is summed and divided by the total number of measurements to generate the average round trip delay.